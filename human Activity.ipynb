{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4c9ff",
   "metadata": {},
   "source": [
    "# <center> BMI 500: Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc43a2f",
   "metadata": {},
   "source": [
    "## Week - 13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239701cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec959e9",
   "metadata": {},
   "source": [
    "### Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f5a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, kernel_size):\n",
    "    p_data = np.zeros(data.shape)\n",
    "    for ch in range(data.shape[1]):\n",
    "        kps_seq_ch = data[:, ch]\n",
    "        kps_seq_ch = Series(kps_seq_ch).rolling(kernel_size, min_periods=1, center=True).mean().to_numpy()\n",
    "        p_data[:, ch] = kps_seq_ch\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158437c",
   "metadata": {},
   "source": [
    "### Segmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22af140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(data, win_len):\n",
    "    win_len = int(30 * win_len)\n",
    "    win_step = int(30 * 0.5)\n",
    "    sample_windows = []\n",
    "\n",
    "    for start_time in range(0, data.shape[0], win_step):\n",
    "        end_time = start_time + win_len\n",
    "        if end_time > data.shape[0]:\n",
    "            end_time = data.shape[0]\n",
    "            start_time = end_time - win_len\n",
    "        frame = data[start_time:end_time]\n",
    "        assert frame.shape[0] == win_len, (start_time, end_time, data.shape[0])\n",
    "        sample_windows.append(frame)\n",
    "\n",
    "    return np.array(sample_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c301caf",
   "metadata": {},
   "source": [
    "### Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd24db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(sample_windows):\n",
    "    N, T, D = sample_windows.shape\n",
    "    feats = []\n",
    "    for i in range(N):\n",
    "        frame = sample_windows[i]\n",
    "        feat = []\n",
    "        for ch in range(D):\n",
    "            frame_ch = frame[:,ch]\n",
    "            # mean feature\n",
    "            mean_ch = np.mean(frame_ch)\n",
    "            feat.append(mean_ch)\n",
    "            # std feature\n",
    "            std_ch = np.std(frame_ch)\n",
    "            feat.append(std_ch)\n",
    "        feats.append(feat)\n",
    "    feats = np.array(feats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d081e",
   "metadata": {},
   "source": [
    "### Loading all the data files from the POSE folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af85cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pose_data(directory):\n",
    "    data_dict = defaultdict(list)\n",
    "    for file_name in os.listdir(directory):\n",
    "        # Extract subject number\n",
    "        subject_number = int(file_name.split('_')[1][1:])\n",
    "        # Extract activity label\n",
    "        label_act = int(file_name.split('_')[0][1:]) - 1\n",
    "\n",
    "        data3D = np.load(os.path.join(directory, file_name))\n",
    "        data = data3D.reshape(data3D.shape[0], -1)\n",
    "        \n",
    "        data_prep = preprocess_data(data, 5)\n",
    "        data_seg = segment_data(data_prep, 1.5)\n",
    "        features = feature_extraction(data_seg)\n",
    "\n",
    "        data_dict[subject_number].append((features, [label_act] * len(features)))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53322b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, kernel_size):\n",
    "    data3D = np.load(file_path)\n",
    "    data = data3D.reshape(data3D.shape[0], -1)\n",
    "    return preprocess_data(data, kernel_size)\n",
    "\n",
    "def extract_and_store_features(data, win_length, subject_number, activity_label, data_dict):\n",
    "    segmented_data = segment_data(data, win_length)\n",
    "    features = feature_extraction(segmented_data)\n",
    "    num_segments = segmented_data.shape[0]\n",
    "    data_dict[subject_number].append((features, [activity_label] * num_segments))\n",
    "\n",
    "def process_pose_files(directory, kernel_size, win_length):\n",
    "    data_dictionary = defaultdict(list)\n",
    "    file_names = os.listdir(directory)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        subject_number = int(file_name[5:7])\n",
    "        activity_label = int(file_name[1:3]) - 1\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        preprocessed_data = load_and_preprocess_data(file_path, kernel_size)\n",
    "        extract_and_store_features(preprocessed_data, win_length, subject_number, activity_label, data_dictionary)\n",
    "\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e1a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "kernel_size = 15\n",
    "win_length = 1.5\n",
    "pose_directory = 'pose'\n",
    "\n",
    "# Process pose data files\n",
    "pose_data_dict = process_pose_files(pose_directory, kernel_size, win_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafc48f",
   "metadata": {},
   "source": [
    "### Spliting the data into TRAIN, VALIDATION and TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e051d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_features(data_dict, subject_range, sample_range):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for subject in subject_range:\n",
    "        for sample in sample_range:\n",
    "            features, labels = data_dict[subject][sample]\n",
    "            features_list.append(features)\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "    stacked_features = np.vstack(features_list)\n",
    "    stacked_labels = np.hstack(labels_list)\n",
    "\n",
    "    return stacked_features, stacked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db58e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges for train, validation, and test sets\n",
    "train_subjects = range(1, 6)\n",
    "val_subjects = range(6, 8)\n",
    "test_subjects = range(8, 11)\n",
    "sample_range = range(32)\n",
    "\n",
    "# Process the data for each set\n",
    "trainx, trainy = stack_features(pose_data_dict, train_subjects, sample_range)\n",
    "valx, valy = stack_features(pose_data_dict, val_subjects, sample_range)\n",
    "testx, testy = stack_features(pose_data_dict, test_subjects, sample_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af7678a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2243, 132), (893, 132), (1156, 132))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape, valx.shape, testx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542d49a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2243,), (893,), (1156,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy.shape, valy.shape, testy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0994b6",
   "metadata": {},
   "source": [
    "### Nenural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a4eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_features, train_labels, val_features, val_labels):\n",
    "    model.fit(train_features, train_labels)\n",
    "    val_predictions = model.predict(val_features)\n",
    "    return accuracy_score(val_labels, val_predictions)\n",
    "\n",
    "def find_hyperparameters(param_grid, trainx, trainy, valx, valy):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        model = MLPClassifier(**param_dict)\n",
    "        val_accuracy = train_and_evaluate(model, trainx, trainy, valx, valy)\n",
    "        if val_accuracy > best_score:\n",
    "            best_score = val_accuracy\n",
    "            best_params = param_dict\n",
    "    return best_score, best_params\n",
    "\n",
    "def nn_model(best_params, trainx, trainy, valx, valy, testx, testy):\n",
    "    trainx_all = np.vstack((trainx, valx))\n",
    "    trainy_all = np.hstack((trainy, valy))\n",
    "    best_model = MLPClassifier(**best_params)\n",
    "    best_model.fit(trainx_all, trainy_all)\n",
    "    test_predictions = best_model.predict(testx)\n",
    "    test_accuracy = accuracy_score(testy, test_predictions)\n",
    "    return best_model, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7eff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.01, 'max_iter': 2000, 'solver': 'adam', 'learning_rate': 'adaptive', 'activation': 'logistic'}\n",
      "Validation Set Accuracy with Best Parameters: 0.6438969764837627\n",
      "Test Set Accuracy with Best Parameters: 0.5544982698961938\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100, 50), (50, ), (100, )],\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [2000],\n",
    "    'solver': ['adam'],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'activation': ['logistic', 'relu']\n",
    "}\n",
    "\n",
    "best_score, best_params = find_hyperparameters(param_grid, trainx, trainy, valx, valy)\n",
    "best_model, test_accuracy = nn_model(best_params, trainx, trainy, valx, valy, testx, testy)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Validation Set Accuracy with Best Parameters:\", best_score)\n",
    "print(\"Test Set Accuracy with Best Parameters:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871772da",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c9d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_find_hyp(param_grid, trainx, trainy, valx, valy):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        model = RandomForestClassifier(**param_dict)\n",
    "        val_accuracy = train_and_evaluate(model, trainx, trainy, valx, valy)\n",
    "\n",
    "        if val_accuracy > best_score:\n",
    "            best_score = val_accuracy\n",
    "            best_params = param_dict\n",
    "\n",
    "    return best_score, best_params\n",
    "\n",
    "def rf_model(best_params, trainx, trainy, valx, valy, testx, testy):\n",
    "    trainx_all = np.vstack((trainx, valx))\n",
    "    trainy_all = np.hstack((trainy, valy))\n",
    "    \n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "    best_model.fit(trainx_all, trainy_all)\n",
    "\n",
    "    test_predictions = best_model.predict(testx)\n",
    "    test_accuracy = accuracy_score(testy, test_predictions)\n",
    "\n",
    "    return best_model, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f05d9fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 30, 'max_depth': 10, 'min_samples_leaf': 5}\n",
      "Validation Set Accuracy with Best Parameters: 0.5487122060470325\n",
      "Test Set Accuracy with Best Parameters: 0.513840830449827\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "n = 10\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 50, 100],\n",
    "    'max_depth': list(np.arange(2, n+1)),\n",
    "    'min_samples_leaf': list(np.arange(2, n+1))\n",
    "}\n",
    "\n",
    "best_score, best_params = rf_find_hyp(param_grid, trainx, trainy, valx, valy)\n",
    "best_model, test_accuracy = rf_model(best_params, trainx, trainy, valx, valy, testx, testy)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Validation Set Accuracy with Best Parameters:\", best_score)\n",
    "print(\"Test Set Accuracy with Best Parameters:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5fe3a",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "I would like to acknowledge the assistance provided by OpenAI's ChatGPT in completing my homework assignment for BMI 500 Week 13. The insights and guidance offered by ChatGPT significantly aided in understanding and solving various aspects of the assignment, particularly in the areas of code development and conceptual explanations. I have thoroughly reviewed and understood all contributions made by ChatGPT and have ensured that they align with the academic integrity standards of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2308c6f",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "BMI 500 Lecture NOtes: \n",
    "https://drive.google.com/drive/folders/13fXFgqlh6m19XOXiFBn1MwsSPKOHpEps\n",
    "\n",
    "Data Repository: \n",
    "https://www.dropbox.com/s/nzhu004aus5sho8/pose.zip?dl=0\n",
    "\n",
    "Pandas: \n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.rolling.html\n",
    "\n",
    "Random Forest Classifier: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Neural Network: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "Accuracy Score:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
